结合 深度学习-Tensorflow 里的一块看
# 不错的参考资料
[LSTM神经网络输入输出究竟是怎样的？](https://www.zhihu.com/question/41949741)
[Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)
[LSTM与GRU数学推导](https://www.jianshu.com/p/1dc21b622cf9)
# 我的理解
LSTM 或者 GRU 都是 Recurrent NNs，我不喜欢叫循环神经网络，因为并没有循环，只是在时间序列 time_step 上的传递延续而已，我觉得叫**递归神经网络**更合适一些。
MLP 是 多层神经网络的意思。
![](./_image/2018-10-27-17-12-28.jpg?r=56)
RNN
![](./_image/2018-10-27-17-12-57.jpg?r=55)
我理解每一个time_step 都是一个MLP，而 Gate 结构控制着黄色隐藏单元的传递逻辑。一般隐藏单元数量和output单元数量相同。
# 核心参数
input_size 输出层的维度，可以理解为 embedding 的维度，有时候也叫 depth
unit_num 隐藏单元个数 == state_size ,一般也 == output_size
max_times 每个 batch 中 time_step的个数，有时候也叫 squence_length。
输入 Input 维度：【batch_size,  squence_length/max_times/time_steps , input_size】
输出 Output 维度：【batch_size, squence_length/max_times/time_steps, output_size】,但是一般 output_size == state_size , 即： output unit 单元的数量 == 隐藏层 state unit 单元的数量。
输出 State 维度：【batch_size,  state_size】，如果是句子的话，相当于每一个的最后一个词的output

state 一般是用来传递到下一个 time_step 的，实际的输出我们一般都是用的 Output。


