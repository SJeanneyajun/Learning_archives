## 问题
1. 注意力权重，即概率分布如何计算
2. 加权求和如何做
3. 怎么在 RNN 中实现
4. 代码实现
## 参考资料
根据这篇文章基本看懂了
[深度学习中的注意力机制](https://blog.csdn.net/qq_40027052/article/details/78421155)
## 需注意
1. 正确理解 RNN 结构很重要，最主要看图一定要理解其关于时间轴展开的形式，如下图，不然很容易陷入误区，不知所云。
![](./_image/2018-10-24-07-53-32.jpg?r=44)
## 问题理解
1. 注意力权重，即概率分布如何计算
![](./_image/2018-10-24-07-54-43.jpg?r=66)
对于采用RNN的Decoder来说，在时刻i，如果要生成yi单词，我们是可以知道Target在生成yi之前的时刻i-1时，隐层节点i-1时刻的输出值Hi-1的，而我们的目的是要计算生成yi时输入句子中的单词“Tom”、“Chase”、“Jerry”对yi来说的注意力分配概率分布，那么可以用Target输出句子i-1时刻的隐层节点状态Hi-1去一一和输入句子Source中每个单词对应的RNN隐层节点状态hj进行对比，即**通过函数F(hj,Hi-1)来获得目标单词yi和每个输入单词对应的对齐可能性**，这个F函数在不同论文里可能会采取不同的方法，然后函数F的输出经过Softmax进行归一化就得到了符合概率分布取值区间的注意力分配概率分布数值。绝大多数Attention模型都是采取上述的计算框架来计算注意力分配概率分布信息，区别只是在F的定义上可能有所不同。
2. 注意力权重怎么用
![](./_image/2018-10-24-07-59-52.jpg?r=72)
